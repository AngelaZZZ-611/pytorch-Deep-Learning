---
lang-ref: ch.05-3
title: Understanding Convolutions and Automatic Gradient
authors: Leyi Zhu, Siqi Wang, Tao Wang, Anqi Zhang
date: 28 Feb 2020
---

## Understanding Matrix-Vector Mulplication

$$
A \boldsymbol{x}=\left[\begin{array}{cccc}
{a_{11}} & {a_{11}} & {\cdots} & {a_{1 n}} \\
{a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\
{\vdots} & {\vdots} & {\ddots} & {\vdots} \\
{a_{m 1}} & {a_{m 2}} & {\cdots} & {a_{m n}}
\end{array}\right]\left[\begin{array}{c}
{x_{1}} \\
{x_{2}} \\
{\vdots} \\
{x_{n}}
\end{array}\right]
$$

There are two different equivalent representation of matrix-vector multiplication. The first one is 
$$
A\boldsymbol{x}=\left[\begin{array}{c}
{\boldsymbol{a}^{(1)} \boldsymbol{x}} \\
{\boldsymbol{a}^{(2)}\boldsymbol{x}} \\
{\vdots} \\
{\boldsymbol{a}^{(m)} \boldsymbol{x}}
\end{array}\right]
$$
Where $\boldsymbol{a}^{(i)}$ is $i$th row vector of matrix $A$ and each $\boldsymbol{a}^{(i)}\boldsymbol{x}$ can be viewed as the **projection** from  input signal space to the kernel. 

The other interpretation is 
$$
A\boldsymbol{x}=\left[\begin{array}{c}
{a_{11}} \\
{a_{21}} \\
{\vdots} \\
{a_{m 1}}
\end{array}\right] x_{1}+\left[\begin{array}{c}
{a_{12}} \\
{a_{22}} \\
{\vdots} \\
{a_{m 2}}
\end{array}\right] x_{2}+\ldots \ldots+\left[\begin{array}{c}
{a_{1 n}} \\
{a_{2 n}} \\
{\vdots} \\
{a_{m n}}
\end{array}\right] x_{n}
$$
this can be viewed as weighted sum of the columns of matrix $A$.

## Understanding 1D Convolution

In this part we will discuss convolution, since we would like to explore the sparsity, stationarity, compositionality of the data.

Instead of using the matrix $A$ above, we will change the matrix width to the kernel size $k$. Therefore, each row of the matrix is a kernel. We can use the kernels by stacking and shifting (see Fig 1). Then we can have $m$ layers of height $n-k+1$. 

![](https://i.imgur.com/KuV1Ikd.png)
<center> 
<img src="../05-3/Illustration_1D_Conv.png" alt="1" style="zoom:100%;" /><br>
<b>Fig 1</b>: Illustration of 1D Convolution  
</center>


The output is $m$ (thickness) vectors of size $n-k+1$.

![2](./2.png)![](https://i.imgur.com/gWhdTpX.png)

<center> 
<img src="../05-3/Result_1D_Conv.png" alt="2" style="zoom:100%;" /><br>
<b>Fig 2</b>: Result of 1D Convolution  
</center>


Furthermore, a single input vector can viewed as a monophonic signal.

![](https://i.imgur.com/6XTpaG2.png) 
<center>
<img src="../05-3/Monophonic_Signal.png" alt="3" style="zoom:100%;" /><br>
<b>Fig 3</b>: Monophonic Signal
</center>


Now, the input $x$ is a mapping
$$
x:\Omega\rightarrow\mathbb{R}^{c}
$$
where $\Omega=\{1,2,3,\dots\}\subset\mathbb{N}^1$ (since this is 1 dimentional signal in a 1 dimentional domain) and in this case channel number $c$ is $1$, and when $c=2$ this becomes a stereophonic system.

For the 1d convolution, we can just compute the scalar product layer by layer. (see Fig 4)
![](https://i.imgur.com/TKHRtu4.png)

<center>
<img src="../05-3/Layer_by_layer_scalar_product.png" alt="4" style="zoom:100%;" /><br>
<b>Fig 4</b>: Layer-by-layer Scalar Product of 1D Convolution
</center>



## Dimension of Kernels and Output Width in PyTorch

Tips: We can use ***question mark*** in IPython to get access to the documents of functions.
![](https://i.imgur.com/uZU4kLA.png)
<center>
<img src="../05-3/Document_Conv1d.png" alt="5" style="zoom:100%;" /><br>
<b>Fig 5</b>: Document of Conv1d Function
</center>


### 1D Convolution

We have 1 dimensional convolution going from 2 channels (stereophonic signal) to 16 channels (16 kernels) with kernel size of 3 and stride of 1. We then have 16 kernels with thickness 2 and length 3. Let's assume that the input signal has a batch of size 1 (one signal),  2 channels and 64 samples. The resulting output layer should have one signal, 16 channels and the length of the signal is 62 (=64-3+1). Also, if we output the bias size, we'll find the bias size is 16, since we have one bias per weight. 
![](https://i.imgur.com/uN2UBLO.png)
<center>
<img src="../05-3/1D_Conv_Example.png" alt="6" style="zoom:100%;" /><br>
<b>Fig 6</b>: 1D Convolution Example: Dimension of Kernels and Output
</center>

### 2D Convolution

We first define the input data as one sample, 20 channels with height 64 and width 128. 2D convolution has 20 channels from input and 16 kernels with size of 3 $\times$ 5. After convolution, the output data has one sample, 16 channels with height 62 (=64-3+1) and width 124 (=128-5+1). 
![](https://i.imgur.com/5FnilYf.png)

<center>
<img src="../05-3/2D_Conv_Example.png" alt="7" style="zoom:100%;" /><br>
<b>Fig 7</b>: 2D Convolution Example: Dimension of Kernels and Output
</center><br>


If we want to achieve the same dimensionality, we can have paddings. Continuing the code above, we can add new parameters to the convolution function: stride=1 and padding=(1, 2) , which means 1 on y direction (1 at the top and 1 at the bottom) and 2 on x direction. Then the output signal is in the same size compared to the input signal. The number of dimensions that is required to store the collection of kernels when you perform 2D convolution is 4.
![](https://i.imgur.com/PfECNiD.png)

<center>
<img src="../05-3/Add_Padding.png" alt="8" style="zoom:100%;" /><br>
<b>Fig 8</b>: Add Padding
</center>


## How Automatic Gradient Works?

In this section we're going to ask torch to check all the computation over the tensors so that we can perform the computation of partial derivatives.  

- Create a 2 $\times$ 2 tensor $\boldsymbol{x}$ with gradient-accumulation capabilities
- Deduct 2 from all elements of $\boldsymbol{x}$ and get $\boldsymbol{y}$ (If we print *y.grad_fn*, we will get *<SubBackward0 object at 0x12904b290>*, which means that y is generated by the module of subtraction $\boldsymbol{x}-2$. Also we can use *y.grad_fn.next_functions\[0][0].variable* to derive the original tensor.)
- Do more operations: $\boldsymbol{z} = 3\boldsymbol{y}^2$
- Calculate the mean of $\boldsymbol{z}$

![](https://i.imgur.com/PlgkYbQ.jpg)
<center>
<img src="../05-3/Flow_Chart.jpg" alt="9" style="zoom:100%;" /><br>
<b>Fig 9</b>: Flow Chart of the Auto-gradient Example
</center><br>

Back propagation is used for computing the gradients. In this example, the process of back propagation can be viewed as computing the gradient $\frac{d\boldsymbol{a}}{d\boldsymbol{x}}$. After computing $\frac{d\boldsymbol{a}}{d\boldsymbol{x}}$ by hand as a validation, we can find that the execution of *a.backward()* gives us the same value of *x.grad* as our computation.
![](https://i.imgur.com/ejx7JTy.png)<br>
<center>
<img src="../05-3/Auto-gradient_PyTorch.png" alt="10" style="zoom:100%;" /><br>
<b>Fig 10</b>: Automatic Gradient by PyTorch
</center>

![](https://i.imgur.com/80rmKNW.png)

<center>
<img src="../05-3/Verify_by_Hand.png" alt="11" style="zoom:114%;" /><br>
<b>Fig 11</b>: Verify the Result by Hand
</center>
